{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sdhilip200/Content-Based-Recommendation---Good-Reads-data/blob/master/Recommendation_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pandas gradio numpy nltk torch sentence_transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MFpf99rpjaTA",
    "outputId": "86c4bb45-1090-4e0f-ec42-b2af00ed183f"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHfkyO_vjdVA"
   },
   "outputs": [],
   "source": [
    "def process_movie_data():\n",
    "    # Reading data from the genre, name, and description columns\n",
    "    wiki = pd.read_csv(\"wiki_movie_plots_deduped.csv\", usecols = ['Title', 'Plot'], low_memory = True, on_bad_lines=\"skip\")\n",
    "    wiki.columns = ['title', 'overview']\n",
    "    imdb = pd.read_csv(\"imdb_25k.csv\", usecols = ['movie title', 'Overview'], low_memory = True, on_bad_lines=\"skip\")\n",
    "    imdb.columns = ['title', 'overview']\n",
    "\n",
    "    # Concatenate the film and book dataframes\n",
    "    movies = pd.concat([wiki, imdb], ignore_index=True).drop_duplicates('title')\n",
    "    movies.dropna(how='any', inplace=True) \n",
    "    movies.info()\n",
    "\n",
    "    # Get list of strings as input\n",
    "    corpus = movies['overview'].to_list()\n",
    "    titles = movies['title'].to_list()\n",
    "\n",
    "    catalogue = list(zip(corpus, titles))\n",
    "    return corpus, catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_corpus(model, corpus):\n",
    "    embedder = SentenceTransformer(model)\n",
    "    print(\"Encoding the corpus. This might take a while\")\n",
    "    corpus_embeddings = embedder.encode(corpus, show_progress_bar=True, convert_to_tensor=True)\n",
    "    return corpus_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings or load existing ones\n",
    "def get_embeds(path, model ='all-MiniLM-L12-v2'):\n",
    "    if not os.path.exists(path):\n",
    "        corpus, catalogue  = process_movie_data()\n",
    "        corpus_embeddings = embed_corpus(model, corpus)       \n",
    "        print(\"Storing file on disc\")\n",
    "        with open(path, \"wb\") as fOut:\n",
    "            pickle.dump({'catalogue': catalogue, 'embeddings': corpus_embeddings}, fOut)\n",
    "    else:\n",
    "        print(\"Loading pre-computed embeddings from disc\")\n",
    "        with open(path, \"rb\") as fIn:\n",
    "            cache_data = pickle.load(fIn)\n",
    "            catalogue = cache_data['catalogue'] \n",
    "            corpus_embeddings = cache_data['embeddings']\n",
    "        print(\"Embeddings loaded\")\n",
    "    return catalogue, corpus_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2eogBr7FsSb"
   },
   "source": [
    "# SBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45392 entries, 0 to 59284\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     45392 non-null  object\n",
      " 1   overview  45392 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.0+ MB\n",
      "Encoding the corpus. This might take a while\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████| 1419/1419 [26:30<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing file on disc\n"
     ]
    }
   ],
   "source": [
    "# Define path for stored embeddings\n",
    "path = \"stored_embed\"\n",
    "catalogue, corpus_embeddings = get_embeds(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3kZjho3-B-K"
   },
   "outputs": [],
   "source": [
    "# Recommending the Top K similar books or movies\n",
    "\n",
    "def recommendations(query, corpus_embeddings=corpus_embeddings, top_k = 3,model ='all-MiniLM-L12-v2'):\n",
    "    '''\n",
    "    Input: query as a single-item list\n",
    "    Finds k nearest descriptions\n",
    "    Output: titles for k nearest descriptions\n",
    "    \n",
    "    '''\n",
    "    embedder = SentenceTransformer(model)\n",
    "    #Compute embeddings for the query\n",
    "    query_embeddings = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    #Compute cosine-similarities for the query with each description\n",
    "    cosine_scores = util.cos_sim(query_embeddings, corpus_embeddings)\n",
    "    scores = cosine_scores[0].tolist() #get a list of scores\n",
    "\n",
    "    #Find the pairs with the highest cosine similarity scores\n",
    "    pairs = [{'index': [i], 'score': scores[i]} for i in range(len(scores)-1)]\n",
    "\n",
    "    #Sort scores in decreasing order\n",
    "    pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    results = \"\"\n",
    "    for pair in pairs[0:top_k]:\n",
    "        i = pair['index'][0]\n",
    "        result = \"<p>Title: {}<br>Description: {}<p>Score: {:.4f}<p>\".format(catalogue[i][1], # try yield?\n",
    "                                                                                         catalogue[i][0],                                                                                         \n",
    "                                                                                         pair['score'])\n",
    "        results += result\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(fn=recommendations, \n",
    "                    inputs=[gr.Textbox(placeholder=\"Enter plot summary here...\")], \n",
    "                    outputs=[ gr.HTML()])\n",
    "\n",
    "demo.launch(share=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM54kSBzET7YOjNkm2GK7HF",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Recommendation_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
